# bert-fine-tuning

# ëª©ì°¨

- [ëª©ì°¨](#ëª©ì°¨)

- [ì„œë¡ ](#ì„œë¡ )
    - [BERTë€ ë¬´ì—‡ì¸ê°€?](#bertë€-ë¬´ì—‡ì¸ê°€)
    - [Fine-Tuningì˜ ì¥ì ](#fine-tuningì˜-ì¥ì )
        - [NLPì—ì„œì˜ ë³€í™”](#nlpì—ì„œì˜-ë³€í™”)
- [1. ì„¤ì •](#1-ì„¤ì •)
    - [1.1. Colabì—ì„œ í•™ìŠµì„ ìœ„í•œ GPU ì‚¬ìš©](#11-colabì—ì„œ-í•™ìŠµì„-ìœ„í•œ-gpu-ì‚¬ìš©)
    - [1.2. Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜](#12-hugging-face-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„¤ì¹˜)
- [2. CoLA ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°](#2-cola-ë°ì´í„°ì…‹-ë¶ˆëŸ¬ì˜¤ê¸°)
    - [2.1. ë‹¤ìš´ë¡œë“œ ë° ì¶”ì¶œ](#21-ë‹¤ìš´ë¡œë“œ-ë°-ì¶”ì¶œ)
    - [2.2. êµ¬ë¬¸ ë¶„ì„](#22-êµ¬ë¬¸-ë¶„ì„)
- [3. í† í°í™” ë° ì…ë ¥ í¬ë§·](#3-í† í°í™”-ë°-ì…ë ¥-í¬ë§·)
    - [3.1. BERT í† í°í™”ê¸°](#31-bert-í† í°í™”ê¸°)
    - [3.2. Required Formatting](#32-required-formatting)
        - [ìŠ¤í˜ì…œ í† í°](#ìŠ¤í˜ì…œ-í† í°)
        - [ë¬¸ì¥ ê¸¸ì´ ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬](#ë¬¸ì¥-ê¸¸ì´-ë°-ì–´í…ì…˜-ë§ˆìŠ¤í¬)
    - [3.3. ë°ì´í„° ì„¸íŠ¸ í† í°í™”](#33-ë°ì´í„°-ì„¸íŠ¸-í† í°í™”)
    - [3.4. í•™ìŠµ ë° ê²€ì¦ ë¶„í• ](#34-í•™ìŠµ-ë°-ê²€ì¦-ë¶„í• )
- [4. ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ](#4-ë¶„ë¥˜-ëª¨ë¸-í•™ìŠµ)
    - [4.1. BertForSequenceClassification](#41-bertforsequenceclassification)
    - [4.2. ìµœì í™” ë° í•™ìŠµ ì†ë„ ìŠ¤ì¼€ì¤„ëŸ¬](#42-ìµœì í™”-ë°-í•™ìŠµ-ì†ë„-ìŠ¤ì¼€ì¤„ëŸ¬)

# ì„œë¡ 

2018ë…„ì€ NLPì—ì„œ íšê¸°ì ì¸ í•´ì˜€ë‹¤. Allen AIì˜ ELMO, OpenAIì˜ Open-GPTì™€ êµ¬ê¸€ì˜ BERTì™€ ê°™ì€ ëª¨ë¸ì€ ì—°êµ¬ìë“¤ì´ ìµœì†Œí•œì˜ fine-tuningìœ¼ë¡œ ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬í•˜ë˜ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆë‹¤. ê·¸ë¦¬ê³  ë” ì ì€ ë°ì´í„°ì™€ ë” ì ì€ ê³„ì‚° ì‹œê°„ìœ¼ë¡œ pre-trainingëœ ëª¨ë¸ì„ ì œê³µí•˜ì—¬ ì‰½ê²Œ fine-tuningëœ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ìƒì„±í•  ìˆ˜ ìˆì—ˆë‹¤.

í•˜ì§€ë§Œ NLPë¥¼ ì‹œì‘í•œ ë§ì€ ì‚¬ëŒë“¤ê³¼ ì‹¬ì§€ì–´ ì¼ë¶€ ìˆ™ë ¨ëœ ì‹¤ë¬´ìë“¤ë„ ì´ëŸ¬í•œ ê°•ë ¥í•œ ëª¨ë¸ì˜ ì´ë¡ ê³¼ ì‹¤ì œ ì ìš©ì€ ì˜ ì´í•´ë˜ì§€ ì•ŠëŠ”ë‹¤.

## BERTë€ ë¬´ì—‡ì¸ê°€?

2018ë…„ ë§ì— ì¶œì‹œëœ BERT(Bidirectional Encoder Representations from Transformers)ëŠ” NLPì—ì„œ transfer learning ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë” ë‚˜ì€ ì´í•´ì™€ ì‹¤ìš©ì ì¸ ì§€ì¹¨ì„ ì œê³µí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì´ë‹¤. BERTëŠ” ì–¸ì–´ í‘œí˜„ì„ pre trainingí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì„ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê±°ë‚˜ ë¶„ë¥˜, ì§ˆì˜ì‘ë‹µ ë“±ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

## Fine-Tuningì˜ ì¥ì 

BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ pre-trainingëœ BERT ëª¨ë¸ì˜ ëì— í•™ìŠµë˜ì§€ ì•Šì€ ë‰´ëŸ° ì¸µì„ ì¶”ê°€í•˜ê³  ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì´ íŠ¹ì • NLP ì‘ì—…ì— íŠ¹í™”ëœ CNN, BiLSTM ë“±ê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨ ì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ ì¢‹ì€ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. ë¹ ë¥¸ ê°œë°œ
    - ì²«ì§¸, pre-trainingëœ BERT ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ì´ë¯¸ ìš°ë¦¬ ì–¸ì–´ì— ëŒ€í•œ ë§ì€ ì •ë³´ë¥¼ ì¸ì½”ë”©í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ fine-tuningëœ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° í›¨ì”¬ ì ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤. ì´ëŠ” ì´ë¯¸ ë„¤íŠ¸ì›Œí¬ì˜ í•˜ë‹¨ ê³„ì¸µì„ ê´‘ë²”ìœ„í•˜ê²Œ í›ˆë ¨í•œ ê²ƒê³¼ ê°™ìœ¼ë©° ë¶„ë¥˜ ì‘ì—…ì— ëŒ€í•œ ê¸°ëŠ¥ìœ¼ë¡œ ì¶œë ¥ì„ ì‚¬ìš©í•˜ë©´ì„œ ì¡°ì •ë§Œ í•˜ë©´ ëœë‹¤.

2. ì ì€ ë°ì´í„°
    - ë˜í•œ pre-trainingëœ ê°€ì¤‘ì¹˜ ë•Œë¬¸ì— ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ì²˜ìŒë¶€í„° êµ¬ì¶•ëœ ëª¨ë¸ì— í•„ìš”í•œ ê²ƒë³´ë‹¤ í›¨ì”¬ ì‘ì€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì‘ì—…ì„ fine-tuningí•  ìˆ˜ ìˆë‹¤. ì²˜ìŒë¶€í„° êµ¬ì¶•ëœ NLP ëª¨ë¸ì˜ ì£¼ìš” ë‹¨ì ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í•©ë¦¬ì ì¸ ì •í™•ë„ë¡œ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ ì¢…ì¢… ì—„ì²­ë‚˜ê²Œ í° ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“œëŠ” ë° ë§ì€ ì‹œê°„ê³¼ ì—ë„ˆì§€ê°€ íˆ¬ì…ë˜ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. BERTë¥¼ fine-tuningí•¨ìœ¼ë¡œì¨ ì´ì œ í›¨ì”¬ ì ì€ ì–‘ì˜ í•™ìŠµ ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë„ë¡ ëª¨ë¸ì„ êµìœ¡í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚  ìˆ˜ ìˆë‹¤.

3. ë” ì¢‹ì€ ê²°ê³¼
    - ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ ê°„ë‹¨í•œ fine-tuning ì ˆì°¨ëŠ” ë¶„ë¥˜, ì–¸ì–´ ì¶”ë¡ , ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±, ì§ˆì˜ ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•œ ìµœì†Œí•œì˜ ì‘ì—…ë³„ ì¡°ì •ìœ¼ë¡œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤. íŠ¹ì • ì‘ì—…ì—ì„œ ì˜ ì‘ë™í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œì‹œëœ ì‚¬ìš©ì ì§€ì • ë° ë•Œë¡œëŠ” ëª¨í˜¸í•œ ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•˜ê¸°ë³´ë‹¤ëŠ” ë‹¨ìˆœíˆ BERTë¥¼ fine-tuningí•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ë˜ëŠ” ìµœì†Œí•œ ë™ì¼í•œ ëŒ€ì•ˆì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.

### NLPì—ì„œì˜ ë³€í™”

transfer learningìœ¼ë¡œì˜ ì´ëŸ¬í•œ ë³€í™”ëŠ” ì´ì „ì— ì»´í“¨í„° ë¹„ì „ì—ì„œ ì¼ì–´ë‚œ ê²ƒê³¼ ê°™ì€ ë³€í™”ì™€ ìœ ì‚¬í•˜ë‹¤. ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ ì¢‹ì€ ë”¥ ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“œëŠ” ê²ƒì€ ìˆ˜ë°±ë§Œ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ í•„ìš”ë¡œ í•˜ë©° í›ˆë ¨í•˜ëŠ” ë° ë§¤ìš° ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤.

ì—°êµ¬ìë“¤ì€ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ê°€ ë‚®ì€ ê³„ì¸µì€ ë‹¨ìˆœí•œ íŠ¹ì§•ì„ ë†’ì€ ê³„ì¸µì€ ì ì  ë” ë³µì¡í•œ íŠ¹ì§•ì„ ê°–ëŠ” ê³„ì¸µì  íŠ¹ì§• í‘œí˜„ì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. ë§¤ë²ˆ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì²˜ìŒë¶€í„° í›ˆë ¨ì‹œí‚¤ëŠ” ëŒ€ì‹ , ì¼ë°˜í™”ëœ ì´ë¯¸ì§€ ê¸°ëŠ¥ì„ ê°€ì§„ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ì˜ í•˜ìœ„ ê³„ì¸µì„ ë³µì‚¬í•˜ê³  ì „ì†¡í•˜ì—¬ ë‹¤ë¥¸ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

pre-trainingëœ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìƒˆë¡œìš´ ì‘ì—…ì„ ìœ„í•´ ì‹ ì†í•˜ê²Œ ì¬í•™ìŠµí•˜ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì²˜ìŒë¶€í„° í›ˆë ¨ì‹œí‚¤ëŠ” ê°’ë¹„ì‹¼ í”„ë¡œì„¸ìŠ¤ë³´ë‹¤ í›¨ì”¬ ë” ë‚˜ì€ ì¶”ê°€ ê³„ì¸µì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ê³§ ì¼ë°˜ì ì¸ ê´€í–‰ì´ ë˜ì—ˆë‹¤.

2018ë…„ ELMO, BERT, ULMFIT, Open-GPTì™€ ê°™ì€ deep pre-trained language modelì˜ ë„ì…ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ì¼ì–´ë‚œ ê²ƒê³¼ ì²˜ëŸ¼ NLPì—ì„œ transfer learningê²ƒê³¼ ë™ì¼í•œ ë³€í™”ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.


# 1. ì„¤ì •

## 1.1. Colabì—ì„œ í•™ìŠµì„ ìœ„í•œ GPU ì‚¬ìš©

Google Colabì€ GPUì™€ TPUë¥¼ ë¬´ë£Œë¡œ ì œê³µí•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì‹ ê²½ë§ì„ í›ˆë ¨í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ìµœì„ ì´ë‹¤.(ì´ ê²½ìš° GPUë¥¼ ì—°ê²°í•  ê²ƒì´ë‹¤), ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ê²ƒì´ë‹¤.

GPUëŠ” ë©”ë‰´ë¡œ ì´ë™í•˜ì—¬ ë‹¤ìŒì„ ì„ íƒí•˜ì—¬ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

`Edit ğŸ¡’ Notebook Settings ğŸ¡’ Hardware accelerator ğŸ¡’ (GPU)`

ê·¸ëŸ° ë‹¤ìŒ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ GPUê°€ ì¸ì‹ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```Python
import tensorflow as tf

# Get the GPU device name.
device_name = tf.test.gpu_device_name()

# The device name should look like the following:
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')
```

```
Found GPU at: /device:GPU:0
```

torchê°€ GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GPUë¥¼ ì¥ì¹˜ë¡œ ì‹ë³„í•˜ê³  ì§€ì •í•´ì•¼ í•œë‹¤. ë‚˜ì¤‘ì—, ìš°ë¦¬ì˜ í›ˆë ¨ ë£¨í”„ì—ì„œ, ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ ì¥ì¹˜ì— ë¡œë“œí•  ê²ƒì´ë‹¤.

```Python
import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")
```

```
There are 1 GPU(s) available.
We will use the GPU: Tesla T4
```

## 1.2. Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

ë‹¤ìŒìœ¼ë¡œ, BERTì™€ í•¨ê»˜ ì‘ì—…í•˜ê¸° ìœ„í•œ pytorch ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” Hugging Faceì˜ [transformers](https://github.com/huggingface/transformers) íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ì‹œë‹¤. (ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” Open AIì˜ GPT ë° GPT-2ì™€ ê°™ì€ ë‹¤ë¥¸ pre-trainingëœ ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)

ìš°ë¦¬ê°€ pytorch ì¸í„°í˜ì´ìŠ¤ë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” ë†’ì€ ìˆ˜ì¤€ì˜ API(ì‚¬ìš©í•˜ê¸° ì‰½ì§€ë§Œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•˜ì§€ ì•ŠìŒ)ì™€ tensorflow ì½”ë“œë•Œë¬¸ì´ë‹¤.

í˜„ì¬, Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” BERTì™€ í•¨ê»˜ ì‘ì—…í•˜ê¸° ìœ„í•œ ê°€ì¥ ë„ë¦¬ ë°›ì•„ë“¤ì—¬ì§€ê³  ê°•ë ¥í•œ pytorch ì¸í„°í˜ì´ìŠ¤ì¸ ê²ƒ ê°™ë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ pre-trainingëœ transformers ëª¨ë¸ì„ ì§€ì›í•  ë¿ë§Œ ì•„ë‹ˆë¼ íŠ¹ì • ì‘ì—…ì— ì í•©í•œ ì´ëŸ¬í•œ ëª¨ë¸ì˜ ì‚¬ì „ êµ¬ì¶•ëœ ìˆ˜ì •ë„ í¬í•¨í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì´ë²ˆ ë¶„ì„ì—ì„œëŠ” `BertForSequenceClassification`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” í† í° ë¶„ë¥˜, ì§ˆë¬¸ ë‹µë³€, ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ ë“±ì„ ìœ„í•œ ì‘ì—…ë³„ í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤. ì´ëŸ¬í•œ ë¯¸ë¦¬ ì‘ì„±ëœ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ëª©ì ì— ë§ê²Œ BERTë¥¼ ìˆ˜ì •í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ê°„ì†Œí™”ë©ë‹ˆë‹¤.

```Python
!pip install transformers
```

```
[ê°„ì†Œí™”ë¥¼ ìœ„í•´ ì´ outputì€ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.]
```


# 2. CoLA ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•´ The Corpus of Linguistic Acceptability(CoLA) ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¬¸ë²•ì ìœ¼ë¡œ ì •í™•í•˜ê±°ë‚˜ ì˜ëª»ëœ ê²ƒìœ¼ë¡œ ë ˆì´ë¸”ì´ ì§€ì •ëœ ì¼ë ¨ì˜ ë¬¸ì¥ì…ë‹ˆë‹¤. 2018ë…„ 5ì›”ì— ì²˜ìŒ ê³µê°œë˜ì—ˆìœ¼ë©° BERTì™€ ê°™ì€ ëª¨ë¸ì´ ê²½ìŸí•˜ëŠ” "GLUE Benchmark"ì— í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

## 2.1. ë‹¤ìš´ë¡œë“œ ë° ì¶”ì¶œ

wget íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ Colab ì¸ìŠ¤í„´ìŠ¤ì˜ íŒŒì¼ ì‹œìŠ¤í…œì— ë°ì´í„°ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

```Python
!pip install wget
```

```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting wget
  Downloading wget-3.2.zip (10 kB)
Building wheels for collected packages: wget
  Building wheel for wget (setup.py) ... done
  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=6fcd1ffa5fc23b7fc7a11fa4963cacd3d4675bf30315606398577e4ae13b318a
  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0
Successfully built wget
Installing collected packages: wget
Successfully installed wget-3.2
```

ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê¹ƒí—ˆë¸Œì˜ [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)ì—ì„œ í˜¸ìŠ¤íŒ…ë©ë‹ˆë‹¤.

```Python
import wget
import os

print('Downloading dataset...')

# The URL for the dataset zip file.
url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'

# Download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')
```

```
Downloading dataset...
```

íŒŒì¼ ì‹œìŠ¤í…œì— ë°ì´í„° ì„¸íŠ¸ì˜ ì••ì¶•ì„ í’‰ë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ Colab ì¸ìŠ¤í„´ìŠ¤ì˜ íŒŒì¼ ì‹œìŠ¤í…œì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```Python
# Unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip
```

```
Archive:  cola_public_1.1.zip
   creating: cola_public/
  inflating: cola_public/README      
   creating: cola_public/tokenized/
  inflating: cola_public/tokenized/in_domain_dev.tsv  
  inflating: cola_public/tokenized/in_domain_train.tsv  
  inflating: cola_public/tokenized/out_of_domain_dev.tsv  
   creating: cola_public/raw/
  inflating: cola_public/raw/in_domain_dev.tsv  
  inflating: cola_public/raw/in_domain_train.tsv  
  inflating: cola_public/raw/out_of_domain_dev.tsv  
```

## 2.2. êµ¬ë¬¸ ë¶„ì„

íŒŒì¼ ì´ë¦„ì„ ë³´ë©´ `tokenized` ë²„ì „ê³¼ `raw` ë²„ì „ì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

pre-trainingëœ BERTë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì´ ì œê³µí•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì „ í† í°í™”ëœ ë²„ì „ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ì´ëŠ” (1) ëª¨ë¸ì´ íŠ¹ì •í•˜ê³  ê³ ì •ëœ ì–´íœ˜ë¥¼ ê°€ì§€ê³  ìˆê³  (2) BERT í† í¬ë‚˜ì´ì €ê°€ OOV(out-of-vocabulary)ë¥¼ ì²˜ë¦¬í•˜ëŠ” íŠ¹ë³„í•œ ë°©ë²•ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.

```Python
import pandas as pd

# Load the dataset into a pandas dataframe.
df = pd.read_csv("./cola_public/raw/in_domain_train.tsv", delimiter='\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])

# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

# Display 10 random rows from the data.
df.sample(10)
```

```
Number of training sentences: 8,551
```

||sentence_source|label|label_notes|sentence|
|:---:|:---:|:---:|:---:|:---:|
|8200|ad03|1|NaN|They kicked themselves|
|3862|ks08|1|NaN|A big green insect flew into the soup.|
|8298|ad03|1|NaN|I often have a cold.|
|6542|g_81|0|*|Which did you buy the table supported the book?|
|722|bc01|0|*|Home was gone by John.|
|3693|ks08|1|NaN|I think that person we met last week is insane.|
|6283|c_13|1|NaN|Kathleen really hates her job.|
|4118|ks08|1|NaN|Do not use these words in the beginning of a s...|
|2592|l-93|1|NaN|Jessica sprayed paint under the table.|
|8194|ad03|0|*|I sent she away.|

ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ê´€ì‹¬ì„ ê°–ëŠ” ë‘ ê°€ì§€ ì†ì„±ì€ `ë¬¸ì¥`ê³¼ ê·¸ `ë¼ë²¨`ì´ë©°, ì´ë¥¼ "ìˆ˜ìš©ì„± íŒë‹¨(acceptability judgment)"(0=ë¶ˆìˆ˜ìš©(unacceptable), 1=ìˆ˜ìš©(acceptable))ì´ë¼ê³  í•œë‹¤.

ì—¬ê¸° ë¬¸ë²•ì ìœ¼ë¡œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë‹¤ì„¯ ê°œì˜ ë¬¸ì¥ì´ ìˆë‹¤. ê°ì • ë¶„ì„ê³¼ ê°™ì€ ê²ƒë³´ë‹¤ ì´ ì‘ì—…ì´ ì–¼ë§ˆë‚˜ ë” ì–´ë ¤ìš´ì§€ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```Python
df.loc[df.label == 0].sample(5)[['sentence', 'label']]
```

||sentence|label|
|:---:|:---:|:---:|
|4867|They investigated.|0|
|200|The more he reads, the more books I wonder to ...|0|
|4593|Any zebras can't fly.|0|
|3226|Cities destroy easily.|0|
|7337|The time elapsed the day.|0|

í•™ìŠµ ì„¸íŠ¸ì˜ ë¬¸ì¥ê³¼ ë ˆì´ë¸”ì„ ìˆ«ì ë°°ì—´ë¡œ ì¶”ì¶œí•´ ë´…ì‹œë‹¤.

```Python
# Get the lists of sentences and their labels.
sentences = df.sentence.values
labels = df.label.values
```


# 3. í† í°í™” ë° ì…ë ¥ í¬ë§·

ì´ ì„¹ì…˜ì—ì„œëŠ” ë°ì´í„° ì„¸íŠ¸ë¥¼ BERTê°€ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•  ê²ƒì´ë‹¤.

## 3.1. BERT í† í°í™”ê¸°

BERTì— í…ìŠ¤íŠ¸ë¥¼ ê³µê¸‰í•˜ë ¤ë©´ í† í°ìœ¼ë¡œ ë¶„í• í•œ ë‹¤ìŒ í† í°ì„ í† í°í™”ê¸° ì–´íœ˜ì˜ ì¸ë±ìŠ¤ì— ë§¤í•‘í•´ì•¼ í•©ë‹ˆë‹¤.

í† í°í™”ëŠ” BERTì— í¬í•¨ëœ í† í°í™”ê¸°ì—ì„œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ ì…€ì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” "ì¼€ì´ìŠ¤ ì—†ëŠ”" ë²„ì „ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

```Python
from transformers import BertTokenizer

# Load the BERT tokenizer.
print('Loading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
```

```
Loading BERT tokenizer...
Downloading: 100%
232k/232k [00:00<00:00, 3.15MB/s]
Downloading: 100%
28.0/28.0 [00:00<00:00, 380B/s]
Downloading: 100%
570/570 [00:00<00:00, 7.98kB/s]
```

ì¶œë ¥ì„ ë³´ê¸° ìœ„í•´ í•œ ë¬¸ì¥ì— í† í°í™”ê¸°ë¥¼ ì ìš©í•´ ë´…ì‹œë‹¤.

```Python
# Print the original sentence.
print(' Original: ', sentences[0])

# Print the sentence split into tokens.
print('Tokenized: ', tokenizer.tokenize(sentences[0]))

# Print the sentence mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))
```

```
 Original:  Our friends won't buy this analysis, let alone the next one we propose.
Tokenized:  ['our', 'friends', 'won', "'", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']
Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]
```

ì‹¤ì œë¡œ ëª¨ë“  ë¬¸ì¥ì„ ë³€í™˜í•  ë•Œ `tokenize`ì™€ `convert_tokens_to_ids`ë¥¼ ë³„ë„ë¡œ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹  `tokenize.encode` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë‹¨ê³„ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— BERTì˜ í¬ë§· ìš”êµ¬ ì‚¬í•­ ì¤‘ ì¼ë¶€ì— ëŒ€í•´ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.

## 3.2. Required Formatting

ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ì‚´í´ë³¼ ëª‡ ê°€ì§€ í•„ìˆ˜ í˜•ì‹ ì§€ì • ë‹¨ê³„ë¥¼ ìƒëµí–ˆìŠµë‹ˆë‹¤.

*ì°¸ê³  ì‚¬í•­: BERTì— ëŒ€í•œ ì…ë ¥ í˜•ì‹ì€ ë‚´ê°€ ë³´ê¸°ì— "ê³¼ì‰ ì§€ì •"ëœ ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. ì¤‘ë³µë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê±°ë‚˜ ë°ì´í„°ì—ì„œ ì‰½ê²Œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ê²ƒì€ ì‚¬ì‹¤ì´ê³ , ë‚˜ëŠ” ë‚´ê°€ BERT ë‚´ë¶€ë¥¼ ë” ê¹Šì´ ì´í•´í•˜ê²Œ ë˜ë©´ ë” ë§ì´ ë  ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤.*

ë‹¤ìŒì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

1. ê° ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì— íŠ¹ë³„í•œ í† í°ì„ ì¶”ê°€í•˜ì„¸ìš”.
2. ëª¨ë“  ë¬¸ì¥ì„ í•˜ë‚˜ì˜ ì¼ì •í•œ ê¸¸ì´ë¡œ íŒ¨ë“œí•˜ê³  ìë¦…ë‹ˆë‹¤.
3. "attention mask"ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í† í°ê³¼ íŒ¨ë”© í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ êµ¬ë³„í•©ë‹ˆë‹¤.

### ìŠ¤í˜ì…œ í† í°

`[SEP]`

ëª¨ë“  ë¬¸ì¥ ëì— ìŠ¤í˜ì…œ `[SEP]` í† í°ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ í† í°ì€ BERTì— ë‘ ê°œì˜ ê°œë³„ ë¬¸ì¥ì´ ì£¼ì–´ì§€ê³  ë¬´ì–¸ê°€ë¥¼ ê²°ì •í•˜ë„ë¡ ìš”ì²­ë˜ëŠ” ë‘ ë¬¸ì¥ ì‘ì—…ì˜ ì•„í‹°íŒ©íŠ¸ì´ë‹¤(ì˜ˆ: ë¬¸ì¥ Aì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ë¬¸ì¥ Bì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆê¹Œ?).

ìš°ë¦¬ê°€ ë‹¨ë¬¸ ì…ë ¥ë§Œ ìˆëŠ”ë° í† í°ì´ ì™œ ì•„ì§ë„ í•„ìš”í•œì§€ëŠ” ì•„ì§ í™•ì‹¤í•˜ì§€ ì•Šì§€ë§Œ, ê·¸ë ‡ìŠµë‹ˆë‹¤!

<br>

`[CLS]`

ë¶„ë¥˜ ì‘ì—…ì˜ ê²½ìš° ëª¨ë“  ë¬¸ì¥ì˜ ì‹œì‘ ë¶€ë¶„ì— ìŠ¤í˜ì…œ `[CLS]` í† í°ì„ ì¶”ê°€í•´ì•¼ í•œë‹¤.

ì´ í† í°ì€ íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤. BERTëŠ” 12ê°œì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í† í° ì„ë² ë”© ëª©ë¡ì„ ê°€ì ¸ì™€ ì¶œë ¥ì— ë™ì¼í•œ ìˆ˜ì˜ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤(ë¬¼ë¡  í”¼ì³ ê°’ì´ ë³€ê²½ëœë‹¤!).

![image](https://user-images.githubusercontent.com/55765292/205487650-7876f63a-e42a-48c6-aede-15939b00a059.png)

ìµœì¢…(12ë²ˆì§¸) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¶œë ¥ì—ì„œ ë¶„ë¥˜ê¸°ëŠ” ì²« ë²ˆì§¸ ì„ë² ë”©([CLS] í† í°ì— í•´ë‹¹)ë§Œ ì‚¬ìš©í•œë‹¤.

> "ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ í† í°ì€ í•­ìƒ ìŠ¤í˜ì…œ ë¶„ë¥˜ í† í°([CLS])ì…ë‹ˆë‹¤. ì´ í† í°ì— í•´ë‹¹í•˜ëŠ” ìµœì¢… ì€ë‹‰ ìƒíƒœëŠ” ë¶„ë¥˜ ì‘ì—…ì˜ ì§‘ê³„ ì‹œí€€ìŠ¤ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤."([BERT ë…¼ë¬¸](https://arxiv.org/pdf/1810.04805.pdf)ì—ì„œ)

ìµœì¢… ì„ë² ë”©ì— ëŒ€í•´ í’€ë§ ì „ëµì„ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ì´ê²ƒì€ í•„ìš”í•˜ì§€ ì•Šë‹¤. BERTëŠ” ë¶„ë¥˜ì—ë§Œ ì´ [CLS] í† í°ì„ ì‚¬ìš©í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ë¶„ë¥˜ ë‹¨ê³„ì— í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¨ì¼ 768 ê°’ ì„ë² ë”© ë²¡í„°ë¡œ ì¸ì½”ë”©í•˜ë„ë¡ ë™ê¸°ë¥¼ ë¶€ì—¬í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤. ìš°ë¦¬ë¥¼ ìœ„í•œ í’€ë§ì€ ì´ë¯¸ ëë‚¬ì–´ìš”!

### ë¬¸ì¥ ê¸¸ì´ ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬

ë°ì´í„° ì„¸íŠ¸ì˜ ë¬¸ì¥ì€ ë¶„ëª…íˆ ë‹¤ì–‘í•œ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë°, BERTëŠ” ì´ê²ƒì„ ì–´ë–»ê²Œ ì²˜ë¦¬í• ê¹Œìš”?

BERTì—ëŠ” ë‘ ê°€ì§€ ì œì•½ ì¡°ê±´ì´ ìˆìŠµë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì¥ì€ ê³ ì •ëœ ë‹¨ì¼ ê¸¸ì´ë¡œ íŒ¨ë”©ë˜ê±°ë‚˜ ì˜ë ¤ì•¼ í•œë‹¤.
- ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ëŠ” 512 í† í°ì…ë‹ˆë‹¤.

íŒ¨ë”©ì€ BERT ì–´íœ˜ì—ì„œ ì¸ë±ìŠ¤ 0ì— ìˆëŠ” ìŠ¤í˜ì…œ `[PAD]` í† í°ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ 8ê°œì˜ í† í° "MAX_LEN"ìœ¼ë¡œ íŒ¨ë”©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/55765292/205487988-f45c3a9f-fbb0-4197-a027-2dbda1f27102.png)

"ì–´í…ì…˜ ë§ˆìŠ¤í¬"ëŠ” ë‹¨ìˆœíˆ ì–´ë–¤ í† í°ì´ íŒ¨ë”©ì´ê³  ì–´ë–¤ í† í°ì´ íŒ¨ë”©ì´ ì•„ë‹Œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 1ê³¼ 0ì˜ ë°°ì—´ì…ë‹ˆë‹¤. (ì•½ê°„ ì¤‘ë³µë˜ëŠ” ê²ƒ ê°™ì§€ ì•Šë‚˜ìš”?) ì´ ë§ˆìŠ¤í¬ëŠ” BERTì˜ "Self-Attention" ë©”ì»¤ë‹ˆì¦˜ì— ì´ëŸ¬í•œ PAD í† í°ì„ ë¬¸ì¥ í•´ì„ì— í†µí•©í•˜ì§€ ë§ë¼ê³  ë§í•œë‹¤.

ê·¸ëŸ¬ë‚˜ ìµœëŒ€ ê¸¸ì´ëŠ” í›ˆë ¨ê³¼ í‰ê°€ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Tesla K80ì˜ ê²½ìš°:

`MAX_LEN = 128 --> Training epochs take ~5:28 each`

`MAX_LEN = 64 --> Training epochs take ~2:57 each`


## 3.3. ë°ì´í„° ì„¸íŠ¸ í† í°í™”

íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëŒ€ë¶€ë¶„ì˜ êµ¬ë¬¸ ë¶„ì„ ë° ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ `encode` í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•  ì¤€ë¹„ê°€ ë˜ê¸° ì „ì— íŒ¨ë”©/ì˜ë¼ë‚´ê¸°ë¥¼ ìœ„í•œ **ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´**ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì•„ë˜ ì…€ì€ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì˜ í† í°í™” íŒ¨ìŠ¤ë¥¼ í•˜ë‚˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```Python
max_len = 0

# For every sentence...
for sent in sentences:

    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    input_ids = tokenizer.encode(sent, add_special_tokens=True)

    # Update the maximum sentence length.
    max_len = max(max_len, len(input_ids))

print('Max sentence length: ', max_len)
```

```
Max sentence length:  47
```

í˜¹ì‹œ ë” ê¸´ ì‹œí—˜ ë¬¸ì¥ì´ ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ ìµœëŒ€ ê¸¸ì´ë¥¼ 64ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

ì´ì œ ì‹¤ì œ í† í°í™”ë¥¼ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

`tokenizer.encode_plus` í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.

- ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
- ìŠ¤í˜ì…œ `[CLS]` ë° `[SEP]` í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
- í† í°ì„ IDì— ë§¤í•‘í•©ë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ì¥ì„ ê°™ì€ ê¸¸ì´ë¡œ íŒ¨ë“œí•˜ê±°ë‚˜ ìë¦…ë‹ˆë‹¤.
- ì‹¤ì œ í† í°ì„ `[PAD]` í† í°ê³¼ ëª…ì‹œì ìœ¼ë¡œ êµ¬ë³„í•˜ëŠ” ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.

ì²˜ìŒ ë„¤ ê°€ì§€ ê¸°ëŠ¥ì€ `tokenizer.encode`ì— ìˆì§€ë§Œ ë‹¤ì„¯ ë²ˆì§¸ í•­ëª©(ì–´í…ì…˜ ë§ˆìŠ¤í¬)ì„ ì–»ê¸° ìœ„í•´ `tokenizer.encode_plus`ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œëŠ” [ì—¬ê¸°](https://huggingface.co/docs/transformers/main_classes/tokenizer?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)ì— ìˆìŠµë‹ˆë‹¤.

```Python
# Tokenize all of the sentences and map the tokens to thier word IDs.
input_ids = []
attention_masks = []

# For every sentence...
for sent in sentences:
    # `encode_plus` will:
    #   (1) Tokenize the sentence.
    #   (2) Prepend the `[CLS]` token to the start.
    #   (3) Append the `[SEP]` token to the end.
    #   (4) Map tokens to their IDs.
    #   (5) Pad or truncate the sentence to `max_length`
    #   (6) Create attention masks for [PAD] tokens.
    encoded_dict = tokenizer.encode_plus(
                        sent,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 64,           # Pad & truncate all sentences.
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    
    # Add the encoded sentence to the list.    
    input_ids.append(encoded_dict['input_ids'])
    
    # And its attention mask (simply differentiates padding from non-padding).
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Print sentence 0, now as a list of IDs.
print('Original: ', sentences[0])
print('Token IDs:', input_ids[0])
```

```
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Original:  Our friends won't buy this analysis, let alone the next one we propose.
Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,
         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0])
```


## 3.4. í•™ìŠµ ë° ê²€ì¦ ë¶„í• 
í•™ìŠµì— 90%, ê²€ì¦ì— 10%ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•™ìŠµ ì„¸íŠ¸ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤.

```Python
from torch.utils.data import TensorDataset, random_split

# Combine the training inputs into a TensorDataset.
dataset = TensorDataset(input_ids, attention_masks, labels)

# Create a 90-10 train-validation split.

# Calculate the number of samples to include in each set.
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size

# Divide the dataset by randomly selecting samples.
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))
```

```
7,695 training samples
  856 validation samples
```

ìš°ë¦¬ëŠ” ë˜í•œ torch DataLoader í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ iteratorë¥¼ ë§Œë“¤ ê²ƒì´ë‹¤. ì´ê²ƒì€ for ë£¨í”„ì™€ ë‹¬ë¦¬ iteratorë¥¼ ì‚¬ìš©í•˜ë©´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ëŠ” ë° ë„ì›€ì´ ëœë‹¤.

```Python
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# The DataLoader needs to know our batch size for training, so we specify it 
# here. For fine-tuning BERT on a specific task, the authors recommend a batch 
# size of 16 or 32.
batch_size = 32

# Create the DataLoaders for our training and validation sets.
# We'll take training samples in random order. 
train_dataloader = DataLoader(
            train_dataset,  # The training samples.
            sampler = RandomSampler(train_dataset), # Select batches randomly
            batch_size = batch_size # Trains with this batch size.
        )

# For validation the order doesn't matter, so we'll just read them sequentially.
validation_dataloader = DataLoader(
            val_dataset, # The validation samples.
            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.
            batch_size = batch_size # Evaluate with this batch size.
        )
```


# 4. ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

ì…ë ¥ ë°ì´í„°ì˜ í˜•ì‹ì´ ì œëŒ€ë¡œ ì§€ì •ë˜ì—ˆìœ¼ë‹ˆ ì´ì œ BERT ëª¨ë¸ì„ fine-tuningí•´ì•¼ í•©ë‹ˆë‹¤.


## 4.1. BertForSequenceClassification

ì´ ì‘ì—…ì„ ìœ„í•´ ë¨¼ì € pre-trainingëœ BERT ëª¨ë¸ì„ ìˆ˜ì •í•˜ì—¬ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¶œë ¥ì„ ì œê³µí•œ ë‹¤ìŒ ì „ì²´ ëª¨ë¸ì´ ì‘ì—…ì— ì í•©í•  ë•Œê¹Œì§€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ê³„ì† í›ˆë ¨í•˜ê³ ì í•œë‹¤.

ê³ ë§™ê²Œë„ huggingface pytorch êµ¬í˜„ì€ ë‹¤ì–‘í•œ NLP ì‘ì—…ì„ ìœ„í•´ ì„¤ê³„ëœ ì¸í„°í˜ì´ìŠ¤ ì„¸íŠ¸ë¥¼ í¬í•¨í•œë‹¤. ì´ëŸ¬í•œ ì¸í„°í˜ì´ìŠ¤ëŠ” ëª¨ë‘ í›ˆë ¨ëœ BERT ëª¨ë¸ ìœ„ì— êµ¬ì¶•ë˜ì§€ë§Œ, ê°ê°ì€ íŠ¹ì • NLP ì‘ì—…ì„ ìˆ˜ìš©í•˜ë„ë¡ ì„¤ê³„ëœ ìµœìƒìœ„ ê³„ì¸µê³¼ ì¶œë ¥ ìœ í˜•ì´ ë‹¤ë¥´ë‹¤.

ë‹¤ìŒì€ fine-tuningì„ ìœ„í•´ ì œê³µë˜ëŠ” í´ë˜ìŠ¤ì˜ ëª©ë¡ì…ë‹ˆë‹¤.

BertModel
BertForPreTraining
BertForMaskedLM
BertForNextSentencePrediction
**BertForSequenceClassification** - ì‚¬ìš©í•  í´ë˜ìŠ¤
BertForTokenClassification
BertForQuestionAnswering

ì´ì— ëŒ€í•œ ë¬¸ì„œëŠ” [ì—¬ê¸°](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ê²ƒì€ ë¬¸ì¥ ë¶„ë¥˜ê¸°ë¡œ ì‚¬ìš©í•  ë¶„ë¥˜ë¥¼ ìœ„í•´ ìœ„ì— ë‹¨ì¼ ì„ í˜• ë ˆì´ì–´ê°€ ì¶”ê°€ëœ ì¼ë°˜ BERT ëª¨ë¸ì´ë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ ê³µê¸‰í•¨ì— ë”°ë¼ pre-trainingëœ BERT ëª¨ë¸ ì „ì²´ì™€ í›ˆë ¨ë˜ì§€ ì•Šì€ ì¶”ê°€ ë¶„ë¥˜ ê³„ì¸µì´ íŠ¹ì • ì‘ì—…ì— ëŒ€í•´ í›ˆë ¨ëœë‹¤.

ëª‡ ê°€ì§€ ë‹¤ë¥¸ pre-trainingëœ BERT ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. "bert-base-ascased"ëŠ” ì†Œë¬¸ìë§Œ ìˆëŠ” ë²„ì „ì„ ì˜ë¯¸í•˜ë©°, ë‘˜ ì¤‘ ì‘ì€ ë²„ì „("base" vs "large")ì…ë‹ˆë‹¤.

`from_pretrained`ì— ëŒ€í•œ ë¬¸ì„œëŠ” [ì—¬ê¸°](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë©°, ì¶”ê°€ ë§¤ê°œ ë³€ìˆ˜ëŠ” [ì—¬ê¸°](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤.

```Python
from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)

# Tell pytorch to run this model on the GPU.
model.cuda()
```

```
[ê°„ì†Œí™”ë¥¼ ìœ„í•´ ì´ outputì€ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.]
```

í˜¸ê¸°ì‹¬ì„ ìœ„í•´, ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì´ë¦„ë³„ë¡œ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤.

ì•„ë˜ ì…€ì—ì„œ ë‹¤ìŒì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì˜ ì´ë¦„ê³¼ ì¹˜ìˆ˜ë¥¼ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.

- ì„ë² ë”© ë ˆì´ì–´
- 12ê°œì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ì¤‘ ì²« ë²ˆì§¸
- ì¶œë ¥ ë ˆì´ì–´

```Python
# Get all of the model's parameters as a list of tuples.
params = list(model.named_parameters())

print('The BERT model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer ====\n')

for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))
```

```
The BERT model has 201 different named parameters.

==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
```

## 4.2. ìµœì í™” ë° í•™ìŠµ ì†ë„ ìŠ¤ì¼€ì¤„ëŸ¬

ì´ì œ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì €ì¥ëœ ëª¨ë¸ ë‚´ì—ì„œ í›ˆë ¨ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì •ì„ ìœ„ ë‹¤ìŒ ê°’ ì¤‘ì—ì„œ ì„ íƒí•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤([BERT ë…¼ë¬¸](https://arxiv.org/pdf/1810.04805.pdf) ë¶€ë¡ A.3):

> - Batch size: 16, 32
> - Learning rate (Adam): 5e-5, 3e-5, 2e-5
> - Number of epochs: 2, 3, 4

ë‹¤ìŒì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.

- ë°°ì¹˜ í¬ê¸°: 32(DataLoaderë¥¼ ìƒì„±í•  ë•Œ ì„¤ì •
- í•™ìŠµë¥ : 2e-5
- lEpochs: 4(ì´ëŠ” ì•„ë§ˆë„ ë„ˆë¬´ ë§ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë  ê²ƒì…ë‹ˆë‹¤â€¦)

ì—¡ì‹¤ë¡  ë§¤ê°œë³€ìˆ˜ `eps = 1e-8` ì€ "êµ¬í˜„ì—ì„œ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•œ ë§¤ìš° ì‘ì€ ìˆ«ì"ì´ë‹¤.

[ì—¬ê¸°](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109) `run_glue.py`ì—ì„œ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```Python
# Note: AdamW is a class from the huggingface library (as opposed to pytorch) 
# I believe the 'W' stands for 'Weight Decay fix"
optimizer = AdamW(model.parameters(),
                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5
                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.
                )

```

```Python
from transformers import get_linear_schedule_with_warmup

# Number of training epochs. The BERT authors recommend between 2 and 4. 
# We chose to run for 4, but we'll see later that this may be over-fitting the
# training data.
epochs = 4

# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0, # Default value in run_glue.py
                                            num_training_steps = total_steps)
```

## 4.3. í•™ìŠµ ë£¨í”„

ì•„ë˜ëŠ” ìš°ë¦¬ì˜ êµìœ¡ ë£¨í”„ì…ë‹ˆë‹¤. ë§ì€ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ë£¨í”„ì˜ ê° íŒ¨ìŠ¤ì— ëŒ€í•´ í•™ìŠµ ë‹¨ê³„ì™€ ê²€ì¦ ë‹¨ê³„ê°€ ìˆìŠµë‹ˆë‹¤.

**í•™ìŠµ:**
- ë°ì´í„° ì¸í’‹ ë° ë ˆì´ë¸” ë°›ê¸°
- ê°€ì†ì„ ìœ„í•´ GPUì— ë°ì´í„° ë¡œë“œ
- ì´ì „ íŒ¨ìŠ¤ì—ì„œ ê³„ì‚°ëœ ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ì§€ì›ë‹ˆë‹¤.
    - íŒŒì´í† ì¹˜ì—ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ì§€ìš°ì§€ ì•ŠëŠ” í•œ ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ë ˆì´ë””ì–¸íŠ¸ê°€ ëˆ„ì ë©ë‹ˆë‹¤(RNN ë“±ì— ìœ ìš©).
- Forward pass(ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ í”¼ë“œ ì…ë ¥ ë°ì´í„°)
- Backward pass(ì—­ì „íŒŒ)
- ë„¤íŠ¸ì›Œí¬ì— Optimizer.step()ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
- ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì¶”ì  ë³€ìˆ˜

**í‰ê°€:**
- ë°ì´í„° ì¸í’‹ ë° ë ˆì´ë¸” ë°›ê¸°
- ê°€ì†ì„ ìœ„í•´ GPUì— ë°ì´í„° ë¡œë“œ
- Forward pass(ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ í”¼ë“œ ì…ë ¥ ë°ì´í„°)
- ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚° ë° ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ì 

PytorchëŠ” ìš°ë¦¬ì—ê²Œ ëª¨ë“  ìƒì„¸í•œ ê³„ì‚°ì„ ìˆ¨ê¸°ì§€ë§Œ, ìš°ë¦¬ëŠ” ê° ë¼ì¸ì—ì„œ ìœ„ì˜ ë‹¨ê³„ ì¤‘ ì–´ë–¤ ê²ƒì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ë¥¼ ì§šê¸° ìœ„í•´ ì½”ë“œì— ì£¼ì„ì„ ë‹¬ì•˜ìŠµë‹ˆë‹¤.

```Python
import numpy as np

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)
```

`hh:mm:ss`ì™€ ê°™ì´ ê²½ê³¼ ì‹œê°„ í˜•ì‹ì„ ì§€ì •í•˜ëŠ” ë„ìš°ë¯¸ ê¸°ëŠ¥

```Python
import time
import datetime

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))
```

í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

```Python
import random
import numpy as np

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128

# Set the seed value all over the place to make this reproducible.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# We'll store a number of quantities such as training and validation loss, 
# validation accuracy, and timings.
training_stats = []

# Measure the total training time for the whole run.
total_t0 = time.time()

# For each epoch...
for epoch_i in range(0, epochs):
    
    # ========================================
    #               Training
    # ========================================
    
    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_train_loss = 0

    # Put the model into training mode. Don't be mislead--the call to 
    # `train` just changes the *mode*, it doesn't *perform* the training.
    # `dropout` and `batchnorm` layers behave differently during training
    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)
    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        # Progress update every 40 batches.
        if step % 40 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)
            
            # Report progress.
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using the 
        # `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        # Always clear any previously calculated gradients before performing a
        # backward pass. PyTorch doesn't do this automatically because 
        # accumulating the gradients is "convenient while training RNNs". 
        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)
        model.zero_grad()        

        # Perform a forward pass (evaluate the model on this training batch).
        # The documentation for this `model` function is here: 
        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        # It returns different numbers of parameters depending on what arguments
        # arge given and what flags are set. For our useage here, it returns
        # the loss (because we provided labels) and the "logits"--the model
        # outputs prior to activation.
        loss, logits = model(b_input_ids, 
                             token_type_ids=None, 
                             attention_mask=b_input_mask, 
                             labels=b_labels)

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value 
        # from the tensor.
        total_train_loss += loss.item()

        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        # The optimizer dictates the "update rule"--how the parameters are
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)            
    
    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))
        
    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()

    # Tracking variables 
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        
        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using 
        # the `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        # Tell pytorch not to bother with constructing the compute graph during
        # the forward pass, since this is only needed for backprop (training).
        with torch.no_grad():        

            # Forward pass, calculate logit predictions.
            # token_type_ids is the same as the "segment ids", which 
            # differentiates sentence 1 and 2 in 2-sentence tasks.
            # The documentation for this `model` function is here: 
            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
            # Get the "logits" output by the model. The "logits" are the output
            # values prior to applying an activation function like the softmax.
            (loss, logits) = model(b_input_ids, 
                                   token_type_ids=None, 
                                   attention_mask=b_input_mask,
                                   labels=b_labels)
            
        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_eval_accuracy += flat_accuracy(logits, label_ids)
        

    # Report the final accuracy for this validation run.
    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

    # Calculate the average loss over all of the batches.
    avg_val_loss = total_eval_loss / len(validation_dataloader)
    
    # Measure how long the validation run took.
    validation_time = format_time(time.time() - t0)
    
    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))
```

```
======== Epoch 1 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:08.
  Batch    80  of    241.    Elapsed: 0:00:17.
  Batch   120  of    241.    Elapsed: 0:00:25.
  Batch   160  of    241.    Elapsed: 0:00:34.
  Batch   200  of    241.    Elapsed: 0:00:42.
  Batch   240  of    241.    Elapsed: 0:00:51.

  Average training loss: 0.50
  Training epcoh took: 0:00:51

Running Validation...
  Accuracy: 0.80
  Validation Loss: 0.45
  Validation took: 0:00:02

======== Epoch 2 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:08.
  Batch    80  of    241.    Elapsed: 0:00:17.
  Batch   120  of    241.    Elapsed: 0:00:25.
  Batch   160  of    241.    Elapsed: 0:00:34.
  Batch   200  of    241.    Elapsed: 0:00:42.
  Batch   240  of    241.    Elapsed: 0:00:51.

  Average training loss: 0.32
  Training epcoh took: 0:00:51

Running Validation...
  Accuracy: 0.81
  Validation Loss: 0.46
  Validation took: 0:00:02

======== Epoch 3 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:08.
  Batch    80  of    241.    Elapsed: 0:00:17.
  Batch   120  of    241.    Elapsed: 0:00:25.
  Batch   160  of    241.    Elapsed: 0:00:34.
  Batch   200  of    241.    Elapsed: 0:00:42.
  Batch   240  of    241.    Elapsed: 0:00:51.

  Average training loss: 0.22
  Training epcoh took: 0:00:51

Running Validation...
  Accuracy: 0.82
  Validation Loss: 0.49
  Validation took: 0:00:02

======== Epoch 4 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:08.
  Batch    80  of    241.    Elapsed: 0:00:17.
  Batch   120  of    241.    Elapsed: 0:00:25.
  Batch   160  of    241.    Elapsed: 0:00:34.
  Batch   200  of    241.    Elapsed: 0:00:42.
  Batch   240  of    241.    Elapsed: 0:00:51.

  Average training loss: 0.16
  Training epcoh took: 0:00:51

Running Validation...
  Accuracy: 0.82
  Validation Loss: 0.55
  Validation took: 0:00:02

Training complete!
Total training took 0:03:30 (h:mm:ss)
```

í•™ìŠµ ê³¼ì •ì˜ ìš”ì•½ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```Python
import pandas as pd

# Display floats with two decimal places.
pd.set_option('precision', 2)

# Create a DataFrame from our training statistics.
df_stats = pd.DataFrame(data=training_stats)

# Use the 'epoch' as the row index.
df_stats = df_stats.set_index('epoch')

# A hack to force the column headers to wrap.
#df = df.style.set_table_styles([dict(selector="th",props=[('max-width', '70px')])])

# Display the table.
df_stats
```

|epoch|Training Loss|Valid. Loss|Valid. Accur.|Training Time|Validation Time|
|:---:|:---:|:---:|:---:|:---:|:---:|
|1|0.50|0.46|0.81|0:01:18|0:00:03|
|2|0.30|0.41|0.84|0:01:18|0:00:03|
|3|0.19|0.48|0.84|0:01:18|0:00:03|
|4|0.13|0.53|0.84|0:01:18|0:00:03|

Training Lossì´ ê° epochì— ë”°ë¼ ê°ì†Œí•˜ëŠ” ë°˜ë©´, Valid. Lossì€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ëª¨ë¸ì„ ë„ˆë¬´ ì˜¤ë«ë™ì•ˆ í›ˆë ¨ì‹œí‚¤ê³  ìˆìœ¼ë©°, í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ì í•©í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.

(ì°¸ê³ ë¡œ, ìš°ë¦¬ëŠ” 7,695ê°œì˜ êµìœ¡ ìƒ˜í”Œê³¼ 856ê°œì˜ ìœ íš¨ì„± ê²€ì‚¬ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì •í™•ë„ì—ì„œëŠ” ì •í™•í•œ ì¶œë ¥ ê°’ì´ ì•„ë‹ˆë¼ ì„ê³„ê°’ì˜ ì–´ëŠ ìª½ì— í•´ë‹¹í•˜ëŠ”ì§€ì— ëŒ€í•´ ì‹ ê²½ì„ ì“°ê¸° ë•Œë¬¸ì— Validation LossëŠ” ì •í™•ë„ë³´ë‹¤ ë” ì •í™•í•œ ì¸¡ì •ê°’ì…ë‹ˆë‹¤.

ìš°ë¦¬ê°€ ì •ë‹µì„ ì˜ˆì¸¡í•˜ê³  ìˆì§€ë§Œ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê²€ì¦ ì†ì‹¤ì€ ì´ë¥¼ í¬ì°©í•˜ì§€ë§Œ ì •í™•ë„ëŠ” ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤.

```Python
import matplotlib.pyplot as plt
% matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(df_stats['Training Loss'], 'b-o', label="Training")
plt.plot(df_stats['Valid. Loss'], 'g-o', label="Validation")

# Label the plot.
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([1, 2, 3, 4])

plt.show()
```

![image](https://user-images.githubusercontent.com/55765292/205912151-3e125cd3-9e50-4466-94c6-749eb532973f.png)